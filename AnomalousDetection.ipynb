{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1IsW1xY4muLmZ_5LQ3i3xUV5Zlraxsdgi",
      "authorship_tag": "ABX9TyNAKrjR257prmQc32CY6juw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ameixa551/AudioRecorder/blob/main/AnomalousDetection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cwofguxQEcu2",
        "outputId": "1bcac471-084d-4398-b094-30d8b8ec4c73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping Keras as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping librosa as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: numpy 1.23.5\n",
            "Uninstalling numpy-1.23.5:\n",
            "  Successfully uninstalled numpy-1.23.5\n",
            "\u001b[33mWARNING: Skipping pandas as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping scikit_learn as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping scipy as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping SoundFile as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Skipping tensorflow_probability as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: tqdm 4.62.3\n",
            "Uninstalling tqdm-4.62.3:\n",
            "  Successfully uninstalled tqdm-4.62.3\n",
            "Collecting Keras (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 1))\n",
            "  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "Collecting librosa (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2))\n",
            "  Using cached librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
            "Collecting numpy (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 3))\n",
            "  Using cached numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "Collecting pandas==1.5.3 (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 4))\n",
            "  Using cached pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "Collecting scikit_learn (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 5))\n",
            "  Using cached scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "Collecting scipy (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 6))\n",
            "  Using cached scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "Collecting SoundFile==0.10.3.post1 (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 7))\n",
            "  Using cached SoundFile-0.10.3.post1-py2.py3-none-any.whl (21 kB)\n",
            "Collecting tensorflow (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8))\n",
            "  Using cached tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
            "Collecting tensorflow_probability (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 9))\n",
            "  Using cached tensorflow_probability-0.20.1-py2.py3-none-any.whl (6.9 MB)\n",
            "Collecting tqdm==4.62.3 (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 10))\n",
            "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 4)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 4)) (2022.7.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from SoundFile==0.10.3.post1->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 7)) (1.15.1)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (0.56.4)\n",
            "INFO: pip is looking at multiple versions of librosa to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting librosa (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2))\n",
            "  Using cached librosa-0.10.0.post1-py3-none-any.whl (252 kB)\n",
            "  Using cached librosa-0.10.0-py3-none-any.whl (252 kB)\n",
            "  Using cached librosa-0.9.2-py3-none-any.whl (214 kB)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (0.4.2)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (1.6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (23.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 5)) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (3.8.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (16.0.0)\n",
            "Collecting numpy (from -r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 3))\n",
            "  Using cached numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (3.3.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (2.13.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (4.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (0.32.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 9)) (2.2.1)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow_probability->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 9)) (0.1.8)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (0.40.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->SoundFile==0.10.3.post1->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 7)) (2.21)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (0.39.1)\n",
            "  Using cached numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
            "Requirement already satisfied: appdirs>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (1.4.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.0->librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (2.27.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (3.4.3)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->pooch>=1.0->librosa->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 2)) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow->-r /content/drive/MyDrive/icassp2023-main/requirements.txt (line 8)) (3.2.2)\n",
            "Installing collected packages: tqdm, numpy, Keras, tensorflow_probability, SoundFile, scipy, pandas, scikit_learn, librosa, tensorflow\n",
            "Successfully installed Keras-2.13.1 SoundFile-0.10.3.post1 librosa-0.9.2 numpy-1.23.5 pandas-1.5.3 scikit_learn-1.3.0 scipy-1.11.1 tensorflow-2.13.0 tensorflow_probability-0.20.1 tqdm-4.62.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "_soundfile",
                  "keras",
                  "numpy",
                  "pandas",
                  "soundfile",
                  "tensorflow",
                  "tensorflow_probability",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package                          Version\n",
            "-------------------------------- ---------------------\n",
            "absl-py                          1.4.0\n",
            "aiohttp                          3.8.4\n",
            "aiosignal                        1.3.1\n",
            "alabaster                        0.7.13\n",
            "albumentations                   1.2.1\n",
            "altair                           4.2.2\n",
            "anyio                            3.7.0\n",
            "appdirs                          1.4.4\n",
            "argon2-cffi                      21.3.0\n",
            "argon2-cffi-bindings             21.2.0\n",
            "array-record                     0.4.0\n",
            "arviz                            0.15.1\n",
            "astropy                          5.2.2\n",
            "astunparse                       1.6.3\n",
            "async-timeout                    4.0.2\n",
            "attrs                            23.1.0\n",
            "audioread                        3.0.0\n",
            "autograd                         1.6.1\n",
            "Babel                            2.12.1\n",
            "backcall                         0.2.0\n",
            "beautifulsoup4                   4.11.2\n",
            "bleach                           6.0.0\n",
            "blis                             0.7.9\n",
            "blosc2                           2.0.0\n",
            "bokeh                            2.4.3\n",
            "branca                           0.6.0\n",
            "build                            0.10.0\n",
            "CacheControl                     0.13.1\n",
            "cached-property                  1.5.2\n",
            "cachetools                       5.3.1\n",
            "catalogue                        2.0.8\n",
            "certifi                          2023.5.7\n",
            "cffi                             1.15.1\n",
            "chardet                          4.0.0\n",
            "charset-normalizer               2.0.12\n",
            "chex                             0.1.7\n",
            "click                            8.1.3\n",
            "click-plugins                    1.1.1\n",
            "cligj                            0.7.2\n",
            "cloudpickle                      2.2.1\n",
            "cmake                            3.25.2\n",
            "cmdstanpy                        1.1.0\n",
            "colorcet                         3.0.1\n",
            "colorlover                       0.3.0\n",
            "community                        1.0.0b1\n",
            "confection                       0.0.4\n",
            "cons                             0.4.6\n",
            "contextlib2                      0.6.0.post1\n",
            "contourpy                        1.1.0\n",
            "convertdate                      2.4.0\n",
            "cufflinks                        0.17.3\n",
            "cupy-cuda11x                     11.0.0\n",
            "cvxopt                           1.3.1\n",
            "cvxpy                            1.3.1\n",
            "cycler                           0.11.0\n",
            "cymem                            2.0.7\n",
            "Cython                           0.29.35\n",
            "dask                             2022.12.1\n",
            "datascience                      0.17.6\n",
            "db-dtypes                        1.1.1\n",
            "dbus-python                      1.2.16\n",
            "debugpy                          1.6.6\n",
            "decorator                        4.4.2\n",
            "defusedxml                       0.7.1\n",
            "distributed                      2022.12.1\n",
            "dlib                             19.24.2\n",
            "dm-tree                          0.1.8\n",
            "docutils                         0.16\n",
            "dopamine-rl                      4.0.6\n",
            "duckdb                           0.8.1\n",
            "earthengine-api                  0.1.357\n",
            "easydict                         1.10\n",
            "ecos                             2.0.12\n",
            "editdistance                     0.6.2\n",
            "en-core-web-sm                   3.5.0\n",
            "entrypoints                      0.4\n",
            "ephem                            4.1.4\n",
            "et-xmlfile                       1.1.0\n",
            "etils                            1.3.0\n",
            "etuples                          0.3.9\n",
            "exceptiongroup                   1.1.1\n",
            "fastai                           2.7.12\n",
            "fastcore                         1.5.29\n",
            "fastdownload                     0.0.7\n",
            "fastjsonschema                   2.17.1\n",
            "fastprogress                     1.0.3\n",
            "fastrlock                        0.8.1\n",
            "filelock                         3.12.2\n",
            "Fiona                            1.9.4.post1\n",
            "firebase-admin                   5.3.0\n",
            "Flask                            2.2.5\n",
            "flatbuffers                      23.5.26\n",
            "flax                             0.6.11\n",
            "folium                           0.14.0\n",
            "fonttools                        4.40.0\n",
            "frozendict                       2.3.8\n",
            "frozenlist                       1.3.3\n",
            "fsspec                           2023.6.0\n",
            "future                           0.18.3\n",
            "gast                             0.4.0\n",
            "gcsfs                            2023.6.0\n",
            "GDAL                             3.3.2\n",
            "gdown                            4.6.6\n",
            "gensim                           4.3.1\n",
            "geographiclib                    2.0\n",
            "geopandas                        0.13.2\n",
            "geopy                            2.3.0\n",
            "gin-config                       0.5.0\n",
            "glob2                            0.7\n",
            "google                           2.0.3\n",
            "google-api-core                  2.11.1\n",
            "google-api-python-client         2.84.0\n",
            "google-auth                      2.17.3\n",
            "google-auth-httplib2             0.1.0\n",
            "google-auth-oauthlib             1.0.0\n",
            "google-cloud-bigquery            3.10.0\n",
            "google-cloud-bigquery-connection 1.12.0\n",
            "google-cloud-bigquery-storage    2.20.0\n",
            "google-cloud-core                2.3.2\n",
            "google-cloud-datastore           2.15.2\n",
            "google-cloud-firestore           2.11.1\n",
            "google-cloud-functions           1.13.0\n",
            "google-cloud-language            2.9.1\n",
            "google-cloud-storage             2.8.0\n",
            "google-cloud-translate           3.11.1\n",
            "google-colab                     1.0.0\n",
            "google-crc32c                    1.5.0\n",
            "google-pasta                     0.2.0\n",
            "google-resumable-media           2.5.0\n",
            "googleapis-common-protos         1.59.1\n",
            "googledrivedownloader            0.4\n",
            "graphviz                         0.20.1\n",
            "greenlet                         2.0.2\n",
            "grpc-google-iam-v1               0.12.6\n",
            "grpcio                           1.56.0\n",
            "grpcio-status                    1.48.2\n",
            "gspread                          3.4.2\n",
            "gspread-dataframe                3.0.8\n",
            "gym                              0.25.2\n",
            "gym-notices                      0.0.8\n",
            "h5netcdf                         1.2.0\n",
            "h5py                             3.8.0\n",
            "holidays                         0.27.1\n",
            "holoviews                        1.15.4\n",
            "html5lib                         1.1\n",
            "httpimport                       1.3.0\n",
            "httplib2                         0.21.0\n",
            "humanize                         4.6.0\n",
            "hyperopt                         0.2.7\n",
            "idna                             3.4\n",
            "imageio                          2.25.1\n",
            "imageio-ffmpeg                   0.4.8\n",
            "imagesize                        1.4.1\n",
            "imbalanced-learn                 0.10.1\n",
            "imgaug                           0.4.0\n",
            "importlib-resources              5.12.0\n",
            "imutils                          0.5.4\n",
            "inflect                          6.0.4\n",
            "iniconfig                        2.0.0\n",
            "intel-openmp                     2023.1.0\n",
            "ipykernel                        5.5.6\n",
            "ipython                          7.34.0\n",
            "ipython-genutils                 0.2.0\n",
            "ipython-sql                      0.4.1\n",
            "ipywidgets                       7.7.1\n",
            "itsdangerous                     2.1.2\n",
            "jax                              0.4.10\n",
            "jaxlib                           0.4.10+cuda11.cudnn86\n",
            "jieba                            0.42.1\n",
            "Jinja2                           3.1.2\n",
            "joblib                           1.2.0\n",
            "jsonpickle                       3.0.1\n",
            "jsonschema                       4.3.3\n",
            "jupyter-client                   6.1.12\n",
            "jupyter-console                  6.1.0\n",
            "jupyter_core                     5.3.1\n",
            "jupyter-server                   1.24.0\n",
            "jupyterlab-pygments              0.2.2\n",
            "jupyterlab-widgets               3.0.7\n",
            "kaggle                           1.5.13\n",
            "keras                            2.13.1\n",
            "kiwisolver                       1.4.4\n",
            "langcodes                        3.3.0\n",
            "lazy_loader                      0.2\n",
            "libclang                         16.0.0\n",
            "librosa                          0.9.2\n",
            "lightgbm                         3.3.5\n",
            "lit                              16.0.6\n",
            "llvmlite                         0.39.1\n",
            "locket                           1.0.0\n",
            "logical-unification              0.4.6\n",
            "LunarCalendar                    0.0.9\n",
            "lxml                             4.9.2\n",
            "Markdown                         3.4.3\n",
            "markdown-it-py                   3.0.0\n",
            "MarkupSafe                       2.1.3\n",
            "matplotlib                       3.7.1\n",
            "matplotlib-inline                0.1.6\n",
            "matplotlib-venn                  0.11.9\n",
            "mdurl                            0.1.2\n",
            "miniKanren                       1.0.3\n",
            "missingno                        0.5.2\n",
            "mistune                          0.8.4\n",
            "mizani                           0.8.1\n",
            "mkl                              2019.0\n",
            "ml-dtypes                        0.2.0\n",
            "mlxtend                          0.14.0\n",
            "more-itertools                   9.1.0\n",
            "moviepy                          1.0.3\n",
            "mpmath                           1.3.0\n",
            "msgpack                          1.0.5\n",
            "multidict                        6.0.4\n",
            "multipledispatch                 0.6.0\n",
            "multitasking                     0.0.11\n",
            "murmurhash                       1.0.9\n",
            "music21                          8.1.0\n",
            "natsort                          8.3.1\n",
            "nbclient                         0.8.0\n",
            "nbconvert                        6.5.4\n",
            "nbformat                         5.9.0\n",
            "nest-asyncio                     1.5.6\n",
            "networkx                         3.1\n",
            "nibabel                          3.0.2\n",
            "nltk                             3.8.1\n",
            "notebook                         6.4.8\n",
            "numba                            0.56.4\n",
            "numexpr                          2.8.4\n",
            "numpy                            1.23.5\n",
            "oauth2client                     4.1.3\n",
            "oauthlib                         3.2.2\n",
            "opencv-contrib-python            4.7.0.72\n",
            "opencv-python                    4.7.0.72\n",
            "opencv-python-headless           4.7.0.72\n",
            "openpyxl                         3.0.10\n",
            "opt-einsum                       3.3.0\n",
            "optax                            0.1.5\n",
            "orbax-checkpoint                 0.2.6\n",
            "osqp                             0.6.2.post8\n",
            "packaging                        23.1\n",
            "palettable                       3.3.3\n",
            "pandas                           1.5.3\n",
            "pandas-datareader                0.10.0\n",
            "pandas-gbq                       0.17.9\n",
            "pandocfilters                    1.5.0\n",
            "panel                            0.14.4\n",
            "param                            1.13.0\n",
            "parso                            0.8.3\n",
            "partd                            1.4.0\n",
            "pathlib                          1.0.1\n",
            "pathy                            0.10.2\n",
            "patsy                            0.5.3\n",
            "pexpect                          4.8.0\n",
            "pickleshare                      0.7.5\n",
            "Pillow                           8.4.0\n",
            "pip                              23.1.2\n",
            "pip-tools                        6.13.0\n",
            "platformdirs                     3.7.0\n",
            "plotly                           5.13.1\n",
            "plotnine                         0.10.1\n",
            "pluggy                           1.2.0\n",
            "polars                           0.17.3\n",
            "pooch                            1.6.0\n",
            "portpicker                       1.5.2\n",
            "prefetch-generator               1.0.3\n",
            "preshed                          3.0.8\n",
            "prettytable                      0.7.2\n",
            "proglog                          0.1.10\n",
            "progressbar2                     4.2.0\n",
            "prometheus-client                0.17.0\n",
            "promise                          2.3\n",
            "prompt-toolkit                   3.0.38\n",
            "prophet                          1.1.4\n",
            "proto-plus                       1.22.3\n",
            "protobuf                         3.20.3\n",
            "psutil                           5.9.5\n",
            "psycopg2                         2.9.6\n",
            "ptyprocess                       0.7.0\n",
            "py-cpuinfo                       9.0.0\n",
            "py4j                             0.10.9.7\n",
            "pyarrow                          9.0.0\n",
            "pyasn1                           0.5.0\n",
            "pyasn1-modules                   0.3.0\n",
            "pycocotools                      2.0.6\n",
            "pycparser                        2.21\n",
            "pyct                             0.5.0\n",
            "pydantic                         1.10.9\n",
            "pydata-google-auth               1.8.0\n",
            "pydot                            1.4.2\n",
            "pydot-ng                         2.0.0\n",
            "pydotplus                        2.0.2\n",
            "PyDrive                          1.3.1\n",
            "pyerfa                           2.0.0.3\n",
            "pygame                           2.4.0\n",
            "Pygments                         2.14.0\n",
            "PyGObject                        3.36.0\n",
            "pymc                             5.1.2\n",
            "PyMeeus                          0.5.12\n",
            "pymystem3                        0.2.0\n",
            "PyOpenGL                         3.1.7\n",
            "pyparsing                        3.1.0\n",
            "pyproj                           3.6.0\n",
            "pyproject_hooks                  1.0.0\n",
            "pyrsistent                       0.19.3\n",
            "PySocks                          1.7.1\n",
            "pytensor                         2.10.1\n",
            "pytest                           7.2.2\n",
            "python-apt                       0.0.0\n",
            "python-dateutil                  2.8.2\n",
            "python-louvain                   0.16\n",
            "python-slugify                   8.0.1\n",
            "python-utils                     3.7.0\n",
            "pytz                             2022.7.1\n",
            "pyviz-comms                      2.3.2\n",
            "PyWavelets                       1.4.1\n",
            "PyYAML                           6.0\n",
            "pyzmq                            23.2.1\n",
            "qdldl                            0.1.7\n",
            "qudida                           0.0.4\n",
            "regex                            2022.10.31\n",
            "requests                         2.27.1\n",
            "requests-oauthlib                1.3.1\n",
            "requests-unixsocket              0.2.0\n",
            "requirements-parser              0.5.0\n",
            "resampy                          0.4.2\n",
            "rich                             13.4.2\n",
            "rpy2                             3.5.5\n",
            "rsa                              4.9\n",
            "scikit-image                     0.19.3\n",
            "scikit-learn                     1.3.0\n",
            "scipy                            1.11.1\n",
            "scs                              3.2.3\n",
            "seaborn                          0.12.2\n",
            "Send2Trash                       1.8.2\n",
            "setuptools                       67.7.2\n",
            "shapely                          2.0.1\n",
            "six                              1.16.0\n",
            "sklearn-pandas                   2.2.0\n",
            "smart-open                       6.3.0\n",
            "sniffio                          1.3.0\n",
            "snowballstemmer                  2.2.0\n",
            "sortedcontainers                 2.4.0\n",
            "SoundFile                        0.10.3.post1\n",
            "soupsieve                        2.4.1\n",
            "soxr                             0.3.5\n",
            "spacy                            3.5.3\n",
            "spacy-legacy                     3.0.12\n",
            "spacy-loggers                    1.0.4\n",
            "Sphinx                           3.5.4\n",
            "sphinxcontrib-applehelp          1.0.4\n",
            "sphinxcontrib-devhelp            1.0.2\n",
            "sphinxcontrib-htmlhelp           2.0.1\n",
            "sphinxcontrib-jsmath             1.0.1\n",
            "sphinxcontrib-qthelp             1.0.3\n",
            "sphinxcontrib-serializinghtml    1.1.5\n",
            "SQLAlchemy                       2.0.16\n",
            "sqlparse                         0.4.4\n",
            "srsly                            2.4.6\n",
            "statsmodels                      0.13.5\n",
            "sympy                            1.11.1\n",
            "tables                           3.8.0\n",
            "tabulate                         0.8.10\n",
            "tblib                            2.0.0\n",
            "tenacity                         8.2.2\n",
            "tensorboard                      2.13.0\n",
            "tensorboard-data-server          0.7.1\n",
            "tensorflow                       2.13.0\n",
            "tensorflow-datasets              4.9.2\n",
            "tensorflow-estimator             2.13.0\n",
            "tensorflow-gcs-config            2.12.0\n",
            "tensorflow-hub                   0.13.0\n",
            "tensorflow-io-gcs-filesystem     0.32.0\n",
            "tensorflow-metadata              1.13.1\n",
            "tensorflow-probability           0.20.1\n",
            "tensorstore                      0.1.38\n",
            "termcolor                        2.3.0\n",
            "terminado                        0.17.1\n",
            "text-unidecode                   1.3\n",
            "textblob                         0.17.1\n",
            "tf-slim                          1.1.0\n",
            "thinc                            8.1.10\n",
            "threadpoolctl                    3.1.0\n",
            "tifffile                         2023.4.12\n",
            "tinycss2                         1.2.1\n",
            "toml                             0.10.2\n",
            "tomli                            2.0.1\n",
            "toolz                            0.12.0\n",
            "torch                            2.0.1+cu118\n",
            "torchaudio                       2.0.2+cu118\n",
            "torchdata                        0.6.1\n",
            "torchsummary                     1.5.1\n",
            "torchtext                        0.15.2\n",
            "torchvision                      0.15.2+cu118\n",
            "tornado                          6.3.1\n",
            "tqdm                             4.62.3\n",
            "traitlets                        5.7.1\n",
            "triton                           2.0.0\n",
            "tweepy                           4.13.0\n",
            "typer                            0.7.0\n",
            "types-setuptools                 68.0.0.1\n",
            "typing_extensions                4.5.0\n",
            "tzdata                           2023.3\n",
            "tzlocal                          5.0.1\n",
            "uritemplate                      4.1.1\n",
            "urllib3                          1.26.16\n",
            "vega-datasets                    0.9.0\n",
            "wasabi                           1.1.2\n",
            "wcwidth                          0.2.6\n",
            "webcolors                        1.13\n",
            "webencodings                     0.5.1\n",
            "websocket-client                 1.6.0\n",
            "Werkzeug                         2.3.6\n",
            "wheel                            0.40.0\n",
            "widgetsnbextension               3.6.4\n",
            "wordcloud                        1.8.2.2\n",
            "wrapt                            1.14.1\n",
            "xarray                           2022.12.0\n",
            "xarray-einstats                  0.5.1\n",
            "xgboost                          1.7.6\n",
            "xlrd                             2.0.1\n",
            "yarl                             1.9.2\n",
            "yellowbrick                      1.5\n",
            "yfinance                         0.2.21\n",
            "zict                             3.0.0\n",
            "zipp                             3.15.0\n"
          ]
        }
      ],
      "source": [
        "#配置环境\n",
        "!pip uninstall -r /content/drive/MyDrive/icassp2023-main/requirements.txt -y\n",
        "!pip install -r /content/drive/MyDrive/icassp2023-main/requirements.txt\n",
        "!pip list"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!/opt/bin/nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uPdZ3CPkUPH",
        "outputId": "319b79fa-7b3b-4640-aea2-b6020d09acc8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul  9 12:29:16 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   39C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#mixup layer\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "class MixupLayer(layers.Layer):\n",
        "    def __init__(self, prob, alpha=1, **kwargs):\n",
        "        super(MixupLayer, self).__init__(**kwargs)\n",
        "        self.prob = prob\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        # get mixup weights\n",
        "        if self.alpha == 1:\n",
        "            #dist = tfp.distributions.Beta(0.5, 0.5)\n",
        "            #l = dist.sample([tf.shape(inputs[0])[0]])\n",
        "            l = tf.random.uniform(shape=[tf.shape(inputs[0])[0]])\n",
        "        X_l = tf.reshape(l, [-1]+[1]*(len(inputs[0].shape)-1))\n",
        "        y_l = tf.reshape(l, [-1]+[1]*(len(inputs[1].shape)-1))\n",
        "\n",
        "        # mixup data\n",
        "        X1 = inputs[0]\n",
        "        X2 = tf.reverse(inputs[0], axis=[0])\n",
        "        X = X1 * X_l + X2 * (1 - X_l)\n",
        "\n",
        "        # mixup labels\n",
        "        y1 = inputs[1]\n",
        "        y2 = tf.reverse(inputs[1], axis=[0])\n",
        "        y = y1 * y_l + y2 * (1 - y_l)\n",
        "        y = tf.math.maximum(y1 * y_l, y2 * (1 - y_l))\n",
        "\n",
        "        # apply mixup or not\n",
        "        dec = tf.dtypes.cast(tf.random.uniform(shape=[tf.shape(inputs[0])[0]]) < self.prob, tf.dtypes.float32)\n",
        "        dec1 = tf.reshape(dec, [-1] + [1] * (len(inputs[0].shape) - 1))\n",
        "        out1 = dec1 * X + (1 - dec1) * inputs[0]\n",
        "        dec2 = tf.reshape(dec, [-1] + [1] * (len(inputs[1].shape) - 1))\n",
        "        out2 = dec2 * y + (1 - dec2) * inputs[1]\n",
        "        outputs = [out1, out2]\n",
        "\n",
        "        # pick output corresponding to training phase\n",
        "        return K.in_train_phase(outputs, inputs, training=training)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'prob': self.prob,\n",
        "            'alpha': self.alpha\n",
        "        }\n",
        "        base_config = super(MixupLayer, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n",
        "\n"
      ],
      "metadata": {
        "id": "zGvM8td5PSA-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#subcluster_adacos\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "\n",
        "class SCAdaCos(tf.keras.layers.Layer):\n",
        "    def __init__(self, n_classes=10, n_subclusters=1, regularizer=None, **kwargs):\n",
        "        super(SCAdaCos, self).__init__(**kwargs)\n",
        "        self.n_classes = n_classes\n",
        "        self.n_subclusters = n_subclusters\n",
        "        self.s_init = math.sqrt(2) * math.log(n_classes*n_subclusters - 1)\n",
        "        self.regularizer = tf.keras.regularizers.get(regularizer)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(SCAdaCos, self).build(input_shape[0])\n",
        "        self.W = self.add_weight(name='W_AdaCos' + str(self.n_classes) + '_' + str(self.n_subclusters),\n",
        "                                 shape=(input_shape[0][-1], self.n_classes*self.n_subclusters),\n",
        "                                 initializer='glorot_uniform',\n",
        "                                 trainable=False,\n",
        "                                 regularizer=self.regularizer)\n",
        "        self.s = self.add_weight(name='s' + str(self.n_classes) + '_' + str(self.n_subclusters),\n",
        "                                 shape=(),\n",
        "                                  initializer=tf.keras.initializers.Constant(self.s_init),\n",
        "                                  trainable=False,\n",
        "                                  aggregation=tf.VariableAggregation.MEAN)\n",
        "\n",
        "    def call(self, inputs, training=None):\n",
        "        x, y1, y2 = inputs\n",
        "        y1_orig = y1\n",
        "        y1 = tf.repeat(y1, repeats=self.n_subclusters, axis=-1)\n",
        "        # normalize feature\n",
        "        x = tf.nn.l2_normalize(x, axis=1)\n",
        "        # normalize weights\n",
        "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
        "        # dot product\n",
        "        logits = x @ W  # same as cos theta\n",
        "        theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
        "\n",
        "        if training:\n",
        "            max_s_logits = tf.reduce_max(self.s * logits)\n",
        "            B_avg = tf.exp(self.s*logits-max_s_logits)\n",
        "            B_avg = tf.reduce_mean(tf.reduce_sum(B_avg, axis=1))\n",
        "            theta_class = tf.reduce_sum(y1 * theta, axis=1) * tf.math.count_nonzero(y1_orig, axis=1, dtype=tf.dtypes.float32)  # take mix-upped angle of mix-upped classes\n",
        "            theta_med = tfp.stats.percentile(theta_class, q=50)  # computes median\n",
        "            self.s.assign(\n",
        "                (max_s_logits + tf.math.log(B_avg)) /\n",
        "                tf.math.cos(tf.minimum(math.pi / 4, theta_med)) + K.epsilon())\n",
        "        logits *= self.s\n",
        "        out = tf.keras.activations.softmax(logits)\n",
        "        out = tf.reshape(out, (-1, self.n_classes, self.n_subclusters))\n",
        "        out = tf.math.reduce_sum(out, axis=2)\n",
        "        return out\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (None, self.n_classes)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'n_classes': self.n_classes,\n",
        "            'regularizer': self.regularizer,\n",
        "            'n_subclusters': self.n_subclusters\n",
        "        }\n",
        "        base_config = super(SCAdaCos, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))"
      ],
      "metadata": {
        "id": "N-DJYPyaPyYk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "import pandas as pd\n",
        "# import numpy as np\n",
        "import numpy as np\n",
        "import keras\n",
        "import os\n",
        "import soundfile as sf\n",
        "import tensorflow as tf\n",
        "import librosa\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scipy.stats import hmean\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "\n",
        "class MagnitudeSpectrogram(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Compute magnitude spectrograms.\n",
        "    https://towardsdatascience.com/how-to-easily-process-audio-on-your-gpu-with-tensorflow-2d9d91360f06\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sample_rate, fft_size, hop_size, f_min=0.0, f_max=None, **kwargs):\n",
        "        super(MagnitudeSpectrogram, self).__init__(**kwargs)\n",
        "        self.sample_rate = sample_rate\n",
        "        self.fft_size = fft_size\n",
        "        self.hop_size = hop_size\n",
        "        self.f_min = f_min\n",
        "        self.f_max = f_max if f_max else sample_rate / 2\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        super(MagnitudeSpectrogram, self).build(input_shape)\n",
        "\n",
        "    def call(self, waveforms):\n",
        "        spectrograms = tf.signal.stft(waveforms,\n",
        "                                      frame_length=self.fft_size,\n",
        "                                      frame_step=self.hop_size,\n",
        "                                      pad_end=False)\n",
        "        magnitude_spectrograms = tf.abs(spectrograms)\n",
        "        magnitude_spectrograms = tf.expand_dims(magnitude_spectrograms, 3)\n",
        "        return magnitude_spectrograms\n",
        "\n",
        "    def get_config(self):\n",
        "        config = {\n",
        "            'fft_size': self.fft_size,\n",
        "            'hop_size': self.hop_size,\n",
        "            'sample_rate': self.sample_rate,\n",
        "            'f_min': self.f_min,\n",
        "            'f_max': self.f_max,\n",
        "        }\n",
        "        config.update(super(MagnitudeSpectrogram, self).get_config())\n",
        "        return config\n",
        "\n",
        "def mixupLoss(y_true, y_pred):\n",
        "    return tf.keras.losses.categorical_crossentropy(y_true=y_pred[:, :, 1], y_pred=y_pred[:, :, 0])\n",
        "\n",
        "\n",
        "def length_norm(mat):\n",
        "    norm_mat = []\n",
        "    for line in mat:\n",
        "        temp = line / np.math.sqrt(sum(np.power(line, 2)))\n",
        "        norm_mat.append(temp)\n",
        "    norm_mat = np.array(norm_mat)\n",
        "    return norm_mat\n",
        "\n",
        "\n",
        "def model_emb_cnn(num_classes, raw_dim, n_subclusters, use_bias=False):\n",
        "    data_input = tf.keras.layers.Input(shape=(raw_dim, 1), dtype='float32')\n",
        "    label_input = tf.keras.layers.Input(shape=(num_classes), dtype='float32')\n",
        "    y = label_input\n",
        "    x = data_input\n",
        "    l2_weight_decay = tf.keras.regularizers.l2(1e-5)\n",
        "    x_mix, y = MixupLayer(prob=1)([x, y])\n",
        "\n",
        "    # FFT\n",
        "    x = tf.keras.layers.Lambda(lambda x: tf.math.abs(tf.signal.fft(tf.complex(x[:,:,0], tf.zeros_like(x[:,:,0])))[:,:int(raw_dim/2)]))(x_mix)\n",
        "    x = tf.keras.layers.Reshape((-1,1))(x)\n",
        "    x = tf.keras.layers.Conv1D(128, 256, strides=64, activation='linear', padding='same',\n",
        "                               kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv1D(128, 64, strides=32, activation='linear', padding='same',\n",
        "                               kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Conv1D(128, 16, strides=4, activation='linear', padding='same',\n",
        "                               kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    x = tf.keras.layers.Flatten()(x)\n",
        "    x = tf.keras.layers.Dense(128, kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dense(128, kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dense(128, kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.Dense(128, kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "\n",
        "    emb_fft = tf.keras.layers.Dense(128, name='emb_fft', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "\n",
        "    # magnitude\n",
        "    x = tf.keras.layers.Reshape((160000,))(x_mix)\n",
        "    x = MagnitudeSpectrogram(16000, 1024, 512, f_max=8000, f_min=200)(x)\n",
        "    x = tf.keras.layers.Lambda(lambda x: x - tf.math.reduce_mean(x, axis=1, keepdims=True))(x) # CMN-like normalization\n",
        "    x = tf.keras.layers.BatchNormalization(axis=-2)(x)\n",
        "\n",
        "    # first block\n",
        "    x = tf.keras.layers.Conv2D(16, 7, strides=2, activation='linear', padding='same',\n",
        "                               kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.ReLU()(x)\n",
        "    x = tf.keras.layers.MaxPooling2D(3, strides=2)(x)\n",
        "\n",
        "    # second block\n",
        "    xr = tf.keras.layers.ReLU()(x)\n",
        "    xr = tf.keras.layers.Conv2D(16, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    xr = tf.keras.layers.ReLU()(xr)\n",
        "    xr = tf.keras.layers.Conv2D(16, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    x = tf.keras.layers.Add()([x, xr])\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    xr = tf.keras.layers.ReLU()(x)\n",
        "    xr = tf.keras.layers.Conv2D(16, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    xr = tf.keras.layers.ReLU()(xr)\n",
        "    xr = tf.keras.layers.BatchNormalization()(xr)\n",
        "    xr = tf.keras.layers.Conv2D(16, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    x = tf.keras.layers.Add()([x, xr])\n",
        "\n",
        "    # third block\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    xr = tf.keras.layers.ReLU()(x)\n",
        "    xr = tf.keras.layers.Conv2D(32, 3, strides=(2, 2), activation='linear', padding='same',\n",
        "                                kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    xr = tf.keras.layers.BatchNormalization()(xr)\n",
        "    xr = tf.keras.layers.ReLU()(xr)\n",
        "    xr = tf.keras.layers.Conv2D(32, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = tf.keras.layers.Conv2D(kernel_size=1, filters=32, strides=1, padding=\"same\",\n",
        "                               kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "    x = tf.keras.layers.Add()([x, xr])\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    xr = tf.keras.layers.ReLU()(x)\n",
        "    xr = tf.keras.layers.Conv2D(32, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    xr = tf.keras.layers.BatchNormalization()(xr)\n",
        "    xr = tf.keras.layers.ReLU()(xr)\n",
        "    xr = tf.keras.layers.Conv2D(32, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    x = tf.keras.layers.Add()([x, xr])\n",
        "\n",
        "    # fourth block\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    xr = tf.keras.layers.ReLU()(x)\n",
        "    xr = tf.keras.layers.Conv2D(64, 3, strides=(2, 2), activation='linear', padding='same',\n",
        "                                kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    xr = tf.keras.layers.BatchNormalization()(xr)\n",
        "    xr = tf.keras.layers.ReLU()(xr)\n",
        "    xr = tf.keras.layers.Conv2D(64, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = tf.keras.layers.Conv2D(kernel_size=1, filters=64, strides=1, padding=\"same\",\n",
        "                               kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "    x = tf.keras.layers.Add()([x, xr])\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    xr = tf.keras.layers.ReLU()(x)\n",
        "    xr = tf.keras.layers.Conv2D(64, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    xr = tf.keras.layers.BatchNormalization()(xr)\n",
        "    xr = tf.keras.layers.ReLU()(xr)\n",
        "    xr = tf.keras.layers.Conv2D(64, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    x = tf.keras.layers.Add()([x, xr])\n",
        "\n",
        "    # fifth block\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    xr = tf.keras.layers.ReLU()(x)\n",
        "    xr = tf.keras.layers.Conv2D(128, 3, strides=(2, 2), activation='linear', padding='same',\n",
        "                                kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    xr = tf.keras.layers.BatchNormalization()(xr)\n",
        "    xr = tf.keras.layers.ReLU()(xr)\n",
        "    xr = tf.keras.layers.Conv2D(128, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    x = tf.keras.layers.MaxPooling2D((2, 2), padding='same')(x)\n",
        "    x = tf.keras.layers.Conv2D(kernel_size=1, filters=128, strides=1, padding=\"same\",\n",
        "                               kernel_regularizer=l2_weight_decay, use_bias=use_bias)(x)\n",
        "    x = tf.keras.layers.Add()([x, xr])\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    xr = tf.keras.layers.ReLU()(x)\n",
        "    xr = tf.keras.layers.Conv2D(128, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    xr = tf.keras.layers.BatchNormalization()(xr)\n",
        "    xr = tf.keras.layers.ReLU()(xr)\n",
        "    xr = tf.keras.layers.Conv2D(128, 3, activation='linear', padding='same', kernel_regularizer=l2_weight_decay, use_bias=use_bias)(xr)\n",
        "    x = tf.keras.layers.Add()([x, xr])\n",
        "\n",
        "    x = tf.keras.layers.MaxPooling2D((10, 1), padding='same')(x)\n",
        "    x = tf.keras.layers.Flatten(name='flat')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    emb_mel = tf.keras.layers.Dense(128, kernel_regularizer=l2_weight_decay, name='emb_mel', use_bias=use_bias)(x)\n",
        "\n",
        "    # prepare output\n",
        "    x = tf.keras.layers.Concatenate(axis=-1)([emb_fft, emb_mel])\n",
        "    output = SCAdaCos(n_classes=num_classes, n_subclusters=n_subclusters, trainable=False)([x, y, label_input])\n",
        "    loss_output = tf.keras.layers.Lambda(lambda x: tf.stack(x, axis=-1))([output, y])\n",
        "\n",
        "    return data_input, label_input, loss_output\n",
        "\n",
        "\n",
        "########################################################################################################################\n",
        "# Load data and compute embeddings\n",
        "########################################################################################################################\n",
        "target_sr = 16000\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "try:\n",
        "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "except:\n",
        "  # Invalid device or cannot modify virtual devices once initialized.\n",
        "  pass\n",
        "\n",
        "# load train data\n",
        "print('Loading train data')\n",
        "categories = os.listdir(\"/content/drive/MyDrive/icassp2023-main/dev_data/\")\n",
        "\n",
        "if os.path.isfile(str(target_sr) + '_train_raw.npy'):\n",
        "    train_raw = np.load(str(target_sr) + '_train_raw.npy')\n",
        "    train_ids = np.load('train_ids.npy')\n",
        "    train_files = np.load('train_files.npy')\n",
        "    train_atts = np.load('train_atts.npy')\n",
        "    train_domains = np.load('train_domains.npy')\n",
        "else:\n",
        "    train_raw = []\n",
        "    train_ids = []\n",
        "    train_files = []\n",
        "    train_atts = []\n",
        "    train_domains = []\n",
        "    dicts = ['/content/drive/MyDrive/icassp2023-main/dev_data/', '/content/drive/MyDrive/icassp2023-main/eval_data/']\n",
        "    eps = 1e-12\n",
        "    for label, category in enumerate(categories):\n",
        "        print(category)\n",
        "        for dict in dicts:\n",
        "            for count, file in tqdm(enumerate(os.listdir(dict + category + \"/train\")),\n",
        "                                    total=len(os.listdir(dict + category + \"/train\"))):\n",
        "                file_path = dict + category + \"/train/\" + file\n",
        "                wav, fs = sf.read(file_path)\n",
        "                raw = librosa.core.to_mono(wav.transpose()).transpose()[:10 * target_sr]\n",
        "                train_raw.append(raw)\n",
        "                train_ids.append(category + '_' + file.split('_')[1])\n",
        "                train_files.append(file_path)\n",
        "                train_domains.append(file.split('_')[2])\n",
        "                train_atts.append('_'.join(file.split('.wav')[0].split('_')[6:]))\n",
        "    # reshape arrays and store\n",
        "    train_ids = np.array(train_ids)\n",
        "    train_files = np.array(train_files)\n",
        "    train_raw = np.expand_dims(np.array(train_raw, dtype=np.float32), axis=-1)\n",
        "    train_atts = np.array(train_atts)\n",
        "    train_domains = np.array(train_domains)\n",
        "    np.save('train_ids.npy', train_ids)\n",
        "    np.save('train_files.npy', train_files)\n",
        "    np.save('train_atts.npy', train_atts)\n",
        "    np.save('train_domains.npy', train_domains)\n",
        "    np.save(str(target_sr) + '_train_raw.npy', train_raw)\n",
        "\n",
        "# load evaluation data\n",
        "print('Loading evaluation data')\n",
        "if os.path.isfile(str(target_sr) + '_eval_raw.npy'):\n",
        "    eval_raw = np.load(str(target_sr) + '_eval_raw.npy')\n",
        "    eval_ids = np.load('eval_ids.npy')\n",
        "    eval_normal = np.load('eval_normal.npy')\n",
        "    eval_files = np.load('eval_files.npy')\n",
        "    eval_atts = np.load('eval_atts.npy')\n",
        "    eval_domains = np.load('eval_domains.npy')\n",
        "else:\n",
        "    eval_raw = []\n",
        "    eval_ids = []\n",
        "    eval_normal = []\n",
        "    eval_files = []\n",
        "    eval_atts = []\n",
        "    eval_domains = []\n",
        "    eps = 1e-12\n",
        "    for label, category in enumerate(categories):\n",
        "        print(category)\n",
        "        for count, file in tqdm(enumerate(os.listdir(\"./dev_data/\" + category + \"/test\")),\n",
        "                                total=len(os.listdir(\"./dev_data/\" + category + \"/test\"))):\n",
        "            file_path = \"./dev_data/\" + category + \"/test/\" + file\n",
        "            wav, fs = sf.read(file_path)\n",
        "            raw = librosa.core.to_mono(wav.transpose()).transpose()[:10 * target_sr]\n",
        "            eval_raw.append(raw)\n",
        "            eval_ids.append(category + '_' + file.split('_')[1])\n",
        "            eval_normal.append(file.split('_test_')[1].split('_')[0] == 'normal')\n",
        "            eval_files.append(file_path)\n",
        "            eval_domains.append(file.split('_')[2])\n",
        "            eval_atts.append('_'.join(file.split('.wav')[0].split('_')[6:]))\n",
        "    # reshape arrays and store\n",
        "    eval_ids = np.array(eval_ids)\n",
        "    eval_normal = np.array(eval_normal)\n",
        "    eval_files = np.array(eval_files)\n",
        "    eval_atts = np.array(eval_atts)\n",
        "    eval_domains = np.array(eval_domains)\n",
        "    eval_raw = np.expand_dims(np.array(eval_raw, dtype=np.float32), axis=-1)\n",
        "    np.save('eval_ids.npy', eval_ids)\n",
        "    np.save('eval_normal.npy', eval_normal)\n",
        "    np.save('eval_files.npy', eval_files)\n",
        "    np.save('eval_atts.npy', eval_atts)\n",
        "    np.save('eval_domains.npy', eval_domains)\n",
        "    np.save(str(target_sr) + '_eval_raw.npy', eval_raw)\n",
        "\n",
        "# load test data\n",
        "print('Loading test data')\n",
        "if os.path.isfile(str(target_sr) + '_test_raw.npy'):\n",
        "    test_raw = np.load(str(target_sr) + '_test_raw.npy')\n",
        "    test_ids = np.load('test_ids.npy')\n",
        "    test_files = np.load('test_files.npy')\n",
        "else:\n",
        "    test_raw = []\n",
        "    test_ids = []\n",
        "    test_files = []\n",
        "    eps = 1e-12\n",
        "    for label, category in enumerate(categories):\n",
        "        print(category)\n",
        "        for count, file in tqdm(enumerate(os.listdir(\"./eval_data/\" + category + \"/test\")),\n",
        "                                total=len(os.listdir(\"./eval_data/\" + category + \"/test\"))):\n",
        "            file_path = \"./eval_data/\" + category + \"/test/\" + file\n",
        "            wav, fs = sf.read(file_path)\n",
        "            raw = librosa.core.to_mono(wav.transpose()).transpose()[:10 * target_sr]\n",
        "            test_raw.append(raw)\n",
        "            test_ids.append(category + '_' + file.split('_')[1])\n",
        "            test_files.append(file_path)\n",
        "    # reshape arrays and store\n",
        "    test_ids = np.array(test_ids)\n",
        "    test_files = np.array(test_files)\n",
        "    test_raw = np.expand_dims(np.array(test_raw, dtype=np.float32), axis=-1)\n",
        "    np.save('test_ids.npy', test_ids)\n",
        "    np.save('test_files.npy', test_files)\n",
        "    np.save(str(target_sr) + '_test_raw.npy', test_raw)\n",
        "\n",
        "# encode ids as labels\n",
        "le_4train = LabelEncoder()\n",
        "\n",
        "source_train = np.array([file.split('_')[3] == 'source' for file in train_files.tolist()])\n",
        "source_eval = np.array([file.split('_')[3] == 'source' for file in eval_files.tolist()])\n",
        "train_ids_4train = np.array(['###'.join([train_ids[k], train_atts[k], str(source_train[k])]) for k in np.arange(train_ids.shape[0])])\n",
        "eval_ids_4train = np.array(['###'.join([eval_ids[k], eval_atts[k], str(source_eval[k])]) for k in np.arange(eval_ids.shape[0])])\n",
        "le_4train.fit(np.concatenate([train_ids_4train, eval_ids_4train], axis=0))\n",
        "num_classes_4train = len(np.unique(np.concatenate([train_ids_4train, eval_ids_4train], axis=0)))\n",
        "train_labels_4train = le_4train.transform(train_ids_4train)\n",
        "eval_labels_4train = le_4train.transform(eval_ids_4train)\n",
        "\n",
        "le = LabelEncoder()\n",
        "train_labels = le.fit_transform(train_ids)\n",
        "eval_labels = le.transform(eval_ids)\n",
        "test_labels = le.transform(test_ids)\n",
        "num_classes = len(np.unique(train_labels))\n",
        "\n",
        "# distinguish between normal and anomalous samples on development set\n",
        "unknown_raw = eval_raw[~eval_normal]\n",
        "unknown_labels = eval_labels[~eval_normal]\n",
        "unknown_labels_4train = eval_labels_4train[~eval_normal]\n",
        "unknown_files = eval_files[~eval_normal]\n",
        "unknown_ids = eval_ids[~eval_normal]\n",
        "unknown_domains = eval_domains[~eval_normal]\n",
        "source_unknown = source_eval[~eval_normal]\n",
        "eval_raw = eval_raw[eval_normal]\n",
        "eval_labels = eval_labels[eval_normal]\n",
        "eval_labels_4train = eval_labels_4train[eval_normal]\n",
        "eval_files = eval_files[eval_normal]\n",
        "eval_ids = eval_ids[eval_normal]\n",
        "eval_domains = eval_domains[eval_normal]\n",
        "source_eval = source_eval[eval_normal]\n",
        "\n",
        "# training parameters\n",
        "batch_size = 64\n",
        "batch_size_test = 64\n",
        "epochs = 10\n",
        "aeons = 1\n",
        "alpha = 1\n",
        "n_subclusters = 16\n",
        "ensemble_size = 5\n",
        "\n",
        "final_results_dev = np.zeros((ensemble_size, 6))\n",
        "final_results_eval = np.zeros((ensemble_size, 6))\n",
        "\n",
        "pred_eval = np.zeros((eval_raw.shape[0], np.unique(train_labels).shape[0]))\n",
        "pred_unknown = np.zeros((unknown_raw.shape[0], np.unique(train_labels).shape[0]))\n",
        "pred_test = np.zeros((test_raw.shape[0], np.unique(train_labels).shape[0]))\n",
        "pred_train = np.zeros((train_labels.shape[0], np.unique(train_labels).shape[0]))\n",
        "\n",
        "for k_ensemble in np.arange(ensemble_size):\n",
        "    # prepare scores and domain info\n",
        "    y_train_cat = keras.utils.np_utils.to_categorical(train_labels, num_classes=num_classes)\n",
        "    y_eval_cat = keras.utils.np_utils.to_categorical(eval_labels, num_classes=num_classes)\n",
        "    y_unknown_cat = keras.utils.np_utils.to_categorical(unknown_labels, num_classes=num_classes)\n",
        "    y_test_cat = keras.utils.np_utils.to_categorical(test_labels, num_classes=num_classes)\n",
        "\n",
        "    y_train_cat_4train = keras.utils.np_utils.to_categorical(train_labels_4train, num_classes=num_classes_4train)\n",
        "    y_eval_cat_4train = keras.utils.np_utils.to_categorical(eval_labels_4train, num_classes=num_classes_4train)\n",
        "    y_unknown_cat_4train = keras.utils.np_utils.to_categorical(unknown_labels_4train, num_classes=num_classes_4train)\n",
        "\n",
        "    # compile model\n",
        "    data_input, label_input, loss_output = model_emb_cnn(num_classes=num_classes_4train,\n",
        "                                                             raw_dim=eval_raw.shape[1], n_subclusters=n_subclusters, use_bias=False)\n",
        "    model = tf.keras.Model(inputs=[data_input, label_input], outputs=[loss_output])\n",
        "    model.compile(loss=[mixupLoss], optimizer=tf.keras.optimizers.Adam())\n",
        "    print(model.summary())\n",
        "    for k in np.arange(aeons):\n",
        "        print('ensemble iteration: ' + str(k_ensemble+1))\n",
        "        print('aeon: ' + str(k+1))\n",
        "        # fit model\n",
        "        weight_path = 'wts_' + str(k+1) + 'k_' + str(target_sr) + '_' + str(k_ensemble+1) + '.h5'\n",
        "        if not os.path.isfile(weight_path):\n",
        "            model.fit(\n",
        "                [train_raw, y_train_cat_4train], y_train_cat_4train, verbose=1,\n",
        "                batch_size=batch_size, epochs=epochs,\n",
        "                validation_data=([eval_raw, y_eval_cat_4train], y_eval_cat_4train))\n",
        "            model.save(weight_path)\n",
        "        else:\n",
        "            model = tf.keras.models.load_model(weight_path,\n",
        "                                               custom_objects={'MixupLayer': MixupLayer, 'mixupLoss': mixupLoss,\n",
        "                                                               'SCAdaCos': SCAdaCos,\n",
        "                                                               'MagnitudeSpectrogram': MagnitudeSpectrogram})\n",
        "\n",
        "        # extract embeddings\n",
        "        emb_model = tf.keras.Model(model.input, model.layers[-3].output)\n",
        "        eval_embs = emb_model.predict([eval_raw, np.zeros((eval_raw.shape[0], num_classes_4train))], batch_size=batch_size)\n",
        "        train_embs = emb_model.predict([train_raw, np.zeros((train_raw.shape[0], num_classes_4train))], batch_size=batch_size)\n",
        "        unknown_embs = emb_model.predict([unknown_raw, np.zeros((unknown_raw.shape[0], num_classes_4train))], batch_size=batch_size)\n",
        "        test_embs = emb_model.predict([test_raw, np.zeros((test_raw.shape[0], num_classes_4train))], batch_size=batch_size)\n",
        "\n",
        "        # length normalization\n",
        "        x_train_ln = length_norm(train_embs)\n",
        "        x_eval_ln = length_norm(eval_embs)\n",
        "        x_test_ln = length_norm(test_embs)\n",
        "        x_unknown_ln = length_norm(unknown_embs)\n",
        "\n",
        "        for j, lab in tqdm(enumerate(np.unique(train_labels))):\n",
        "            cat = le.inverse_transform([lab])[0]\n",
        "\n",
        "            # prepare mean values for domains\n",
        "            kmeans = KMeans(n_clusters=n_subclusters, random_state=0).fit(x_train_ln[(train_labels == lab)])\n",
        "            means_source_ln = kmeans.cluster_centers_\n",
        "            means_target_ln = x_train_ln[~source_train * (train_labels == lab)]\n",
        "\n",
        "            # compute cosine distances\n",
        "            eval_cos = -np.max(np.dot(x_eval_ln[eval_labels == lab], means_target_ln.transpose()),axis=-1, keepdims=True)\n",
        "            eval_cos = np.minimum(eval_cos, -np.max(np.dot(x_eval_ln[eval_labels == lab], means_source_ln.transpose()), axis=-1, keepdims=True))\n",
        "            unknown_cos = -np.max(np.dot(x_unknown_ln[unknown_labels == lab], means_target_ln.transpose()),axis=-1, keepdims=True)\n",
        "            unknown_cos = np.minimum(unknown_cos, -np.max(np.dot(x_unknown_ln[unknown_labels == lab], means_source_ln.transpose()), axis=-1, keepdims=True))\n",
        "            test_cos = -np.max(np.dot(x_test_ln[test_labels==lab], means_target_ln.transpose()), axis=-1, keepdims=True)\n",
        "            test_cos = np.minimum(test_cos, -np.max(np.dot(x_test_ln[test_labels==lab], means_source_ln.transpose()), axis=-1, keepdims=True))\n",
        "            if np.sum(eval_labels==lab)>0:\n",
        "                pred_eval[eval_labels == lab, j] += np.min(eval_cos, axis=-1)\n",
        "                pred_unknown[unknown_labels == lab, j] += np.min(unknown_cos, axis=-1)\n",
        "            if np.sum(test_labels==lab)>0:\n",
        "                pred_test[test_labels == lab, j] += np.min(test_cos, axis=-1)\n",
        "        # print results for development set\n",
        "        print('#######################################################################################################')\n",
        "        print('DEVELOPMENT SET')\n",
        "        print('#######################################################################################################')\n",
        "        aucs = []\n",
        "        p_aucs = []\n",
        "        aucs_source = []\n",
        "        p_aucs_source = []\n",
        "        aucs_target = []\n",
        "        p_aucs_target = []\n",
        "        for j, cat in enumerate(np.unique(eval_ids)):\n",
        "            y_pred = np.concatenate([pred_eval[eval_labels == le.transform([cat]), le.transform([cat])],\n",
        "                                     pred_unknown[unknown_labels == le.transform([cat]), le.transform([cat])]],\n",
        "                                     axis=0)\n",
        "            y_true = np.concatenate([np.zeros(np.sum(eval_labels == le.transform([cat]))),\n",
        "                                     np.ones(np.sum(unknown_labels == le.transform([cat])))], axis=0)\n",
        "            auc = roc_auc_score(y_true, y_pred)\n",
        "            aucs.append(auc)\n",
        "            p_auc = roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
        "            p_aucs.append(p_auc)\n",
        "            print('AUC for category ' + str(cat) + ': ' + str(auc * 100))\n",
        "            print('pAUC for category ' + str(cat) + ': ' + str(p_auc * 100))\n",
        "\n",
        "            source_all = np.concatenate([source_eval[eval_labels == le.transform([cat])],\n",
        "                                         source_unknown[unknown_labels == le.transform([cat])]], axis=0)\n",
        "            auc = roc_auc_score(y_true[source_all], y_pred[source_all])\n",
        "            p_auc = roc_auc_score(y_true[source_all], y_pred[source_all], max_fpr=0.1)\n",
        "            aucs_source.append(auc)\n",
        "            p_aucs_source.append(p_auc)\n",
        "            print('AUC for source domain of category ' + str(cat) + ': ' + str(auc * 100))\n",
        "            print('pAUC for source domain of category ' + str(cat) + ': ' + str(p_auc * 100))\n",
        "            auc = roc_auc_score(y_true[~source_all], y_pred[~source_all])\n",
        "            p_auc = roc_auc_score(y_true[~source_all], y_pred[~source_all], max_fpr=0.1)\n",
        "            aucs_target.append(auc)\n",
        "            p_aucs_target.append(p_auc)\n",
        "            print('AUC for target domain of category ' + str(cat) + ': ' + str(auc * 100))\n",
        "            print('pAUC for target domain of category ' + str(cat) + ': ' + str(p_auc * 100))\n",
        "        print('####################')\n",
        "        aucs = np.array(aucs)\n",
        "        p_aucs = np.array(p_aucs)\n",
        "        for cat in categories:\n",
        "            mean_auc = hmean(aucs[np.array([eval_id.split('_')[0] for eval_id in np.unique(eval_ids)]) == cat])\n",
        "            print('mean AUC for category ' + str(cat) + ': ' + str(mean_auc * 100))\n",
        "            mean_p_auc = hmean(p_aucs[np.array([eval_id.split('_')[0] for eval_id in np.unique(eval_ids)]) == cat])\n",
        "            print('mean pAUC for category ' + str(cat) + ': ' + str(mean_p_auc * 100))\n",
        "        print('####################')\n",
        "        for cat in categories:\n",
        "            mean_auc = hmean(aucs[np.array([eval_id.split('_')[0] for eval_id in np.unique(eval_ids)]) == cat])\n",
        "            mean_p_auc = hmean(p_aucs[np.array([eval_id.split('_')[0] for eval_id in np.unique(eval_ids)]) == cat])\n",
        "            print('mean of AUC and pAUC for category ' + str(cat) + ': ' + str((mean_p_auc + mean_auc) * 50))\n",
        "        print('####################')\n",
        "        mean_auc_source = hmean(aucs_source)\n",
        "        print('mean AUC for source domain: ' + str(mean_auc_source * 100))\n",
        "        mean_p_auc_source = hmean(p_aucs_source)\n",
        "        print('mean pAUC for source domain: ' + str(mean_p_auc_source * 100))\n",
        "        mean_auc_target = hmean(aucs_target)\n",
        "        print('mean AUC for target domain: ' + str(mean_auc_target * 100))\n",
        "        mean_p_auc_target = hmean(p_aucs_target)\n",
        "        print('mean pAUC for target domain: ' + str(mean_p_auc_target * 100))\n",
        "        mean_auc = hmean(aucs)\n",
        "        print('mean AUC: ' + str(mean_auc * 100))\n",
        "        mean_p_auc = hmean(p_aucs)\n",
        "        print('mean pAUC: ' + str(mean_p_auc * 100))\n",
        "        final_results_dev[k_ensemble] = np.array([mean_auc_source, mean_p_auc_source, mean_auc_target, mean_p_auc_target, mean_auc, mean_p_auc])\n",
        "\n",
        "\n",
        "        # print results for eval set\n",
        "        print('#######################################################################################################')\n",
        "        print('EVALUATION SET')\n",
        "        print('#######################################################################################################')\n",
        "        aucs = []\n",
        "        p_aucs = []\n",
        "        aucs_source = []\n",
        "        p_aucs_source = []\n",
        "        aucs_target = []\n",
        "        p_aucs_target = []\n",
        "        for j, cat in enumerate(np.unique(test_ids)):\n",
        "            y_pred = pred_test[test_labels == le.transform([cat]), le.transform([cat])]\n",
        "            y_true = np.array(pd.read_csv(\n",
        "                './dcase2022_evaluator-main/ground_truth_data/ground_truth_' + cat.split('_')[0] + '_section_' + cat.split('_')[1] + '_test.csv', header=None).iloc[:, 1] == 1)\n",
        "            auc = roc_auc_score(y_true, y_pred)\n",
        "            aucs.append(auc)\n",
        "            p_auc = roc_auc_score(y_true, y_pred, max_fpr=0.1)\n",
        "            p_aucs.append(p_auc)\n",
        "            print('AUC for category ' + str(cat) + ': ' + str(auc * 100))\n",
        "            print('pAUC for category ' + str(cat) + ': ' + str(p_auc * 100))\n",
        "            source_all = np.array(pd.read_csv(\n",
        "                './dcase2022_evaluator-main/ground_truth_domain/ground_truth_' + cat.split('_')[0] + '_section_' + cat.split('_')[1] + '_test.csv', header=None).iloc[:, 1] == 0)\n",
        "            auc = roc_auc_score(y_true[source_all], y_pred[source_all])\n",
        "            p_auc = roc_auc_score(y_true[source_all], y_pred[source_all], max_fpr=0.1)\n",
        "            aucs_source.append(auc)\n",
        "            p_aucs_source.append(p_auc)\n",
        "            print('AUC for source domain of category ' + str(cat) + ': ' + str(auc * 100))\n",
        "            print('pAUC for source domain of category ' + str(cat) + ': ' + str(p_auc * 100))\n",
        "            auc = roc_auc_score(y_true[~source_all], y_pred[~source_all])\n",
        "            p_auc = roc_auc_score(y_true[~source_all], y_pred[~source_all], max_fpr=0.1)\n",
        "            aucs_target.append(auc)\n",
        "            p_aucs_target.append(p_auc)\n",
        "            print('AUC for target domain of category ' + str(cat) + ': ' + str(auc * 100))\n",
        "            print('pAUC for target domain of category ' + str(cat) + ': ' + str(p_auc * 100))\n",
        "        print('####################')\n",
        "        aucs = np.array(aucs)\n",
        "        p_aucs = np.array(p_aucs)\n",
        "        for cat in categories:\n",
        "            mean_auc = hmean(aucs[np.array([eval_id.split('_')[0] for eval_id in np.unique(eval_ids)]) == cat])\n",
        "            print('mean AUC for category ' + str(cat) + ': ' + str(mean_auc * 100))\n",
        "            mean_p_auc = hmean(p_aucs[np.array([eval_id.split('_')[0] for eval_id in np.unique(eval_ids)]) == cat])\n",
        "            print('mean pAUC for category ' + str(cat) + ': ' + str(mean_p_auc * 100))\n",
        "        print('####################')\n",
        "        for cat in categories:\n",
        "            mean_auc = hmean(aucs[np.array([eval_id.split('_')[0] for eval_id in np.unique(eval_ids)]) == cat])\n",
        "            mean_p_auc = hmean(p_aucs[np.array([eval_id.split('_')[0] for eval_id in np.unique(eval_ids)]) == cat])\n",
        "            print('mean of AUC and pAUC for category ' + str(cat) + ': ' + str((mean_p_auc + mean_auc) * 50))\n",
        "        print('####################')\n",
        "        mean_auc_source = hmean(aucs_source)\n",
        "        print('mean AUC for source domain: ' + str(mean_auc_source * 100))\n",
        "        mean_p_auc_source = hmean(p_aucs_source)\n",
        "        print('mean pAUC for source domain: ' + str(mean_p_auc_source * 100))\n",
        "        mean_auc_target = hmean(aucs_target)\n",
        "        print('mean AUC for target domain: ' + str(mean_auc_target * 100))\n",
        "        mean_p_auc_target = hmean(p_aucs_target)\n",
        "        print('mean pAUC for target domain: ' + str(mean_p_auc_target * 100))\n",
        "        mean_auc = hmean(aucs)\n",
        "        print('mean AUC: ' + str(mean_auc * 100))\n",
        "        mean_p_auc = hmean(p_aucs)\n",
        "        print('mean pAUC: ' + str(mean_p_auc * 100))\n",
        "        final_results_eval[k_ensemble] = np.array([mean_auc_source, mean_p_auc_source, mean_auc_target, mean_p_auc_target, mean_auc, mean_p_auc])\n",
        "\n",
        "        # create challenge submission files\n",
        "        print('creating submission files')\n",
        "        sub_path = './teams/submission/team_fkie'\n",
        "        if not os.path.exists(sub_path):\n",
        "            os.makedirs(sub_path)\n",
        "        for j, cat in enumerate(np.unique(test_ids)):\n",
        "            # anomaly scores\n",
        "            file_idx = test_labels == le.transform([cat])\n",
        "            results_an = pd.DataFrame()\n",
        "            results_an['output1'], results_an['output2'] = [[f.split('/')[-1] for f in test_files[file_idx]],\n",
        "                                                            [str(s) for s in pred_test[file_idx, le.transform([cat])]]]\n",
        "            results_an.to_csv(sub_path + '/anomaly_score_' + cat.split('_')[0] + '_section_' + cat.split('_')[-1] + '_test.csv',\n",
        "                              encoding='utf-8', index=False, header=False)\n",
        "\n",
        "            # decision results\n",
        "            train_scores = pred_train[train_labels == le.transform([cat]), le.transform([cat])]\n",
        "            threshold = np.percentile(train_scores, q=90)\n",
        "            decisions = pred_test[file_idx, le.transform([cat])] > threshold\n",
        "            results_dec = pd.DataFrame()\n",
        "            results_dec['output1'], results_dec['output2'] = [[f.split('/')[-1] for f in test_files[file_idx]],\n",
        "                                                              [str(int(s)) for s in decisions]]\n",
        "            results_dec.to_csv(sub_path + '/decision_result_' + cat.split('_')[0] + '_section_' + cat.split('_')[-1] + '_test.csv',\n",
        "                               encoding='utf-8', index=False, header=False)\n",
        "\n",
        "print('####################')\n",
        "print('####################')\n",
        "print('####################')\n",
        "print('final results for development set')\n",
        "print(np.round(np.mean(final_results_dev*100, axis=0), 2))\n",
        "print(np.round(np.std(final_results_dev*100, axis=0), 2))\n",
        "print('final results for evaluation set')\n",
        "print(np.round(np.mean(final_results_eval*100, axis=0), 2))\n",
        "print(np.round(np.std(final_results_eval*100, axis=0), 2))\n",
        "\n",
        "print('####################')\n",
        "print('>>>> finished! <<<<<')\n",
        "print('####################')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r8UgEAojPb5H",
        "outputId": "823dd8ad-93e3-433a-8514-63abbcdc355a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading train data\n",
            "ToyCar\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3000/3000 [00:37<00:00, 80.65it/s] \n",
            "100%|██████████| 3000/3000 [00:40<00:00, 74.27it/s] \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ToyTrain\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 67%|██████▋   | 2001/3000 [00:43<00:11, 90.24it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "j2biEgo5uPu0",
        "outputId": "070fea65-3e90-42b9-c8db-8b3b2e66635b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}